{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**DAAPP Architecture for the Iris Dataset**\n",
        "In this case, we developed a data acquisition and preprocessing pipeline using the Iris data set. The pipeline involves four main stages: This models are Data Acquisition, Feature Extraction, Data Transformation and Data Loading.\n",
        "\n",
        "1. **Data Acquisition:**\n",
        "The Iris dataset was uploaded using the sklearn.datasets library. It contains information about three species of flowers: All three species of iris: setosa, versicolor, and virginica. This dataset is the features that have sepal length, sepal width, petal length, and petal width measurements. These were stored in a Pandas DataFrame for future analysis.\n",
        "\n",
        "2. **Feature Extraction:**\n",
        "From the dataset, we selected three columns: The data set comprises of four measurements: sepal length, sepal width and the label indicating the species. This step helps to commodify the dataset for analysis by only selecting the necessary features together with the target.\n",
        "\n",
        "3. **Data Transformation:**\n",
        "To make the preprocessing of the numerical features more intuitive I standardized the sepal length and width features to the range of [0, 1] using Min-Max scaling. This is important in order to make sure that all features are scaled appropriately and in order that are important for subsequent analysis and machine learning algorithms are properly balanced.\n",
        "\n",
        "4. **Data Loading:**\n",
        "The transformed data were dumped into SQLite database iris_data.db.\n",
        "\n",
        "**Testing:**\n",
        "To confirm the pipeline, unit tests were used for selective columns for the feature extraction function. Furthermore, to ensure that pipeline functionality is correct in terms of loading its data and storing it in the database, an integration test was performed.\n",
        "\n",
        "**Summary of Results:**\n",
        "The pipeline successfully processes the Iris dataset by:\n",
        "\n",
        "**Transferring the data into the Dataframe.**\n",
        "When all the features needed for the performance of further actions are extracted and transformed.\n",
        "Ingesting them into SQLite database after applying cleaning and normalization operations on these data.\n",
        "This process generated a normally structured iris_data.db database which can be used for further analysis or modeling."
      ],
      "metadata": {
        "id": "g9AdanjDwkBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas sqlalchemy kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om-kEux1quxl",
        "outputId": "0fed1c4c-7985-4ec5-da66-2e22e8739e16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.10/dist-packages (2.0.36)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy) (3.1.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Acquiring Data**"
      ],
      "metadata": {
        "id": "A1nTxEUau2Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "def load_iris_data():\n",
        "    iris = load_iris(as_frame=True)\n",
        "    data = iris.data\n",
        "    data['species'] = iris.target\n",
        "    data['species'] = data['species'].map(dict(enumerate(iris.target_names)))\n",
        "    return data\n",
        "\n",
        "# sample use\n",
        "iris_data = load_iris_data()\n",
        "print(iris_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKgjduZftLpj",
        "outputId": "578a3a4f-c631-4932-c53d-29ca6e70f4f2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "  species  \n",
            "0  setosa  \n",
            "1  setosa  \n",
            "2  setosa  \n",
            "3  setosa  \n",
            "4  setosa  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Selection**"
      ],
      "metadata": {
        "id": "BYdEC25XvC72"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcEsWH7GpUM-",
        "outputId": "8c7d5bad-3e05-44d8-db0e-c481189ab4c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sepal length (cm)  sepal width (cm) species\n",
            "0                5.1               3.5  setosa\n",
            "1                4.9               3.0  setosa\n",
            "2                4.7               3.2  setosa\n",
            "3                4.6               3.1  setosa\n",
            "4                5.0               3.6  setosa\n"
          ]
        }
      ],
      "source": [
        "def extract_features(df):\n",
        "    selected_columns = ['sepal length (cm)', 'sepal width (cm)', 'species']\n",
        "    return df[selected_columns]\n",
        "\n",
        "# Sample use\n",
        "extracted_data = extract_features(iris_data)\n",
        "print(extracted_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Transformation**"
      ],
      "metadata": {
        "id": "nt8SbNlhvJv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def transform_data(df):\n",
        "    features = df.select_dtypes(include=['float64'])\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_features = scaler.fit_transform(features)\n",
        "    scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
        "    scaled_df['species'] = df['species']\n",
        "    return scaled_df\n",
        "\n",
        "# sample use\n",
        "transformed_data = transform_data(extracted_data)\n",
        "print(transformed_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKcvNH6BqIav",
        "outputId": "72722b54-148e-4a12-aec8-794004d74e68"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sepal length (cm)  sepal width (cm) species\n",
            "0           0.222222          0.625000  setosa\n",
            "1           0.166667          0.416667  setosa\n",
            "2           0.111111          0.500000  setosa\n",
            "3           0.083333          0.458333  setosa\n",
            "4           0.194444          0.666667  setosa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Data to Database**"
      ],
      "metadata": {
        "id": "eXXU6z7-vQmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine\n",
        "\n",
        "def load_to_database(df, db_name=\"iris_data.db\", table_name=\"iris\"):\n",
        "    engine = create_engine(f\"sqlite:///{db_name}\")\n",
        "    df.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
        "    print(f\"Data loaded into table '{table_name}' in database '{db_name}'.\")\n",
        "\n",
        "# Sample use\n",
        "load_to_database(transformed_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQqsLd4mqLUt",
        "outputId": "bcfb1e30-0f13-4e77-b15b-3d0a8a135208"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded into table 'iris' in database 'iris_data.db'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unit Test for Feature Extraction**"
      ],
      "metadata": {
        "id": "FzTKAwievfUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_extract_features():\n",
        "    sample_data = pd.DataFrame({\n",
        "        'sepal length (cm)': [5.1, 4.9],\n",
        "        'sepal width (cm)': [3.5, 3.0],\n",
        "        'petal length (cm)': [1.4, 1.4],\n",
        "        'petal width (cm)': [0.2, 0.2],\n",
        "        'species': ['setosa', 'setosa']\n",
        "    })\n",
        "    output = extract_features(sample_data)\n",
        "    assert list(output.columns) == ['sepal length (cm)', 'sepal width (cm)', 'species']\n",
        "    print(\"Feature extraction test passed!\")\n",
        "\n",
        "test_extract_features()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqO0EX4jqQse",
        "outputId": "dc3cbac5-14a3-4052-9573-db69aa1dbe56"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extraction test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integration Test for the Pipeline**"
      ],
      "metadata": {
        "id": "TcZ_ZIbPvqiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_pipeline():\n",
        "    data = load_iris_data()\n",
        "    features = extract_features(data)\n",
        "    transformed_data = transform_data(features)\n",
        "    load_to_database(transformed_data)\n",
        "    print(\"Pipeline test passed!\")\n",
        "\n",
        "test_pipeline()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64rpdyWsumDg",
        "outputId": "f765f35b-f1f9-42d6-f98d-2cf918674c5d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded into table 'iris' in database 'iris_data.db'.\n",
            "Pipeline test passed!\n"
          ]
        }
      ]
    }
  ]
}