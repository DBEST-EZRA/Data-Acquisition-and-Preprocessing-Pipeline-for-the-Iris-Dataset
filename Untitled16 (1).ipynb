{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing necessary Libraries**"
      ],
      "metadata": {
        "id": "yce-xqS1Ijp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sqlalchemy import create_engine"
      ],
      "metadata": {
        "id": "jC3Upxb4V31X"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scraping Books Data**"
      ],
      "metadata": {
        "id": "ONUNlyWPIg-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_books_to_scrape():\n",
        "    url = \"http://books.toscrape.com/\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check response code\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error: Unable to access site. Status code: {response.status_code}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    books = []\n",
        "    rows = soup.select('article.product_pod')\n",
        "\n",
        "    for row in rows[:50]:\n",
        "        try:\n",
        "            title = row.select_one('h3 a')['title']\n",
        "            price = row.select_one('.price_color').text.strip()\n",
        "            rating = row.select_one('p.star-rating')['class'][1]\n",
        "            books.append({\"title\": title, \"price\": price, \"rating\": rating})\n",
        "        except AttributeError as e:\n",
        "            print(f\"Error parsing row: {row}\")\n",
        "            print(e)\n",
        "\n",
        "    # Create DataFrame\n",
        "    book_df = pd.DataFrame(books)\n",
        "    return book_df"
      ],
      "metadata": {
        "id": "Qvl7tH-iV-fA"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cleaning the Scraped Data**"
      ],
      "metadata": {
        "id": "s_oac2ASIYtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df):\n",
        "    # Removing duplicates\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    # Missing values\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Trimming whitespace\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        df[col] = df[col].str.strip()\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Tc3u-baHWVaE"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Extraction and Data Transformation**"
      ],
      "metadata": {
        "id": "NqDdK_X-JDHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_data(df):\n",
        "    # Convert price to numeric\n",
        "    df['price'] = df['price'].str.replace('£', '', regex=False).str.replace(',', '', regex=False).str.replace('Â', '', regex=False)\n",
        "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
        "\n",
        "    # Normalize price\n",
        "    scaler = MinMaxScaler()\n",
        "    df['price_normalized'] = scaler.fit_transform(df[['price']])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "5eE3Eww8Wqph"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving to CSV**"
      ],
      "metadata": {
        "id": "FOpH00epZppA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_csv(df, file_name=\"books_data.csv\"):\n",
        "    if not df.empty:\n",
        "        df.to_csv(file_name, index=False)\n",
        "        print(f\"Data saved to {file_name}\")\n",
        "    else:\n",
        "        print(\"Error: DataFrame is empty. No data saved to CSV.\")"
      ],
      "metadata": {
        "id": "XNuiJrl8ZlEG"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving to SQLite Database**"
      ],
      "metadata": {
        "id": "796jEcI8JVxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_to_database(df, db_name=\"books_data.db\", table_name=\"top_books\"):\n",
        "    engine = create_engine(f\"sqlite:///{db_name}\")\n",
        "    df.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
        "    print(f\"Data loaded into table '{table_name}' in database '{db_name}'.\")"
      ],
      "metadata": {
        "id": "NspqgmbzXHum"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unit Testing**"
      ],
      "metadata": {
        "id": "GmFA-P3kdqSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_clean_data():\n",
        "    raw_data = scrape_books_to_scrape()\n",
        "\n",
        "    if raw_data.empty:\n",
        "        print(\"Error: No data scraped.\")\n",
        "        return\n",
        "\n",
        "    cleaned_data = clean_data(raw_data)\n",
        "\n",
        "    assert cleaned_data.shape[0] == raw_data.shape[0], f\"Test failed: Expected {raw_data.shape[0] - 1} rows after cleaning, got {cleaned_data.shape[0]}\"\n",
        "    assert cleaned_data['title'].isnull().sum() == 0, \"Test failed: Missing title value after cleaning\"\n",
        "\n",
        "    print(\"Data cleaning test passed!\")\n",
        "\n",
        "test_clean_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y00LLolycA9x",
        "outputId": "0d03c47d-318e-4d02-f5fd-f67254aa66ed"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data cleaning test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integration Test**"
      ],
      "metadata": {
        "id": "mRHWbrbYdwnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_pipeline():\n",
        "    print(\"Starting pipeline test...\")\n",
        "\n",
        "    raw_data = scrape_books_to_scrape()\n",
        "\n",
        "    if raw_data.empty:\n",
        "        print(\"No data scraped. Exiting test.\")\n",
        "        return\n",
        "\n",
        "    cleaned_data = clean_data(raw_data)\n",
        "\n",
        "    transformed_data = transform_data(cleaned_data)\n",
        "\n",
        "    save_to_csv(transformed_data)\n",
        "    load_to_database(transformed_data)\n",
        "\n",
        "    print(\"Pipeline test passed!\")\n",
        "\n",
        "test_pipeline()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyj4CBmab71p",
        "outputId": "4ef78f6f-9d6b-4216-9835-dff038bccda5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting pipeline test...\n",
            "Data saved to books_data.csv\n",
            "Data loaded into table 'top_books' in database 'books_data.db'.\n",
            "Pipeline test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calling the functions**"
      ],
      "metadata": {
        "id": "85V1mbenJhkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def full_pipeline():\n",
        "    raw_data = scrape_books_to_scrape()\n",
        "\n",
        "    if raw_data.empty:\n",
        "        print(\"No data scraped. Exiting pipeline.\")\n",
        "        return\n",
        "\n",
        "    cleaned_data = clean_data(raw_data)\n",
        "\n",
        "    transformed_data = transform_data(cleaned_data)\n",
        "\n",
        "    save_to_csv(transformed_data)\n",
        "\n",
        "    load_to_database(transformed_data)\n",
        "\n",
        "full_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qi9kW0-XPLf",
        "outputId": "8e33fde2-28bb-486c-ca6f-81f7b22650df"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to books_data.csv\n",
            "Data loaded into table 'top_books' in database 'books_data.db'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M2g7xG58b59E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}